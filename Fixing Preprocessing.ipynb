{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "altered-tooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from batchgenerators.utilities.file_and_folder_operations import *\n",
    "import SimpleITK as sitk\n",
    "import nibabel as nib\n",
    "import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "base_path = \"C:/Users/JiachennCJC/Documents/GitHub/brats-pretraining_jiachenn/\"\n",
    "brats_folder_with_training_data_2020 = f\"{base_path}MICCAI_BraTS2020_TrainingData/\"\n",
    "brats_folder_with_validation_data_2020 = f\"{base_path}MICCAI_BraTS2020_ValidationData/\"\n",
    "training_data_path = brats_folder_with_training_data_2020\n",
    "validation_data_path = brats_folder_with_validation_data_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "several-claim",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to C:\\Users\\JiachennCJC/.cache\\torch\\checkpoints\\resnet34-333f7ec4.pth\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8c382cef6e02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjonas_net\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAlbuNet3D34\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAlbuNet3D34\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\brats-pretraining_jiachenn\\jonas_net.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, num_classes, num_filters, pretrained, is_deconv)\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;31m#self.encoder = models.resnet18(pretrained=pretrained)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresnet34\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m         \u001b[1;31m#self.encoder = models.resnet50(pretrained=pretrained)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[1;31m#self.encoder = models.resnet101(pretrained=pretrained)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FYP\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mresnet34\u001b[1;34m(pretrained, progress, **kwargs)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[0mprogress\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisplays\u001b[0m \u001b[0ma\u001b[0m \u001b[0mprogress\u001b[0m \u001b[0mbar\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdownload\u001b[0m \u001b[0mto\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \"\"\"\n\u001b[1;32m--> 248\u001b[1;33m     return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress,\n\u001b[0m\u001b[0;32m    249\u001b[0m                    **kwargs)\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FYP\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36m_resnet\u001b[1;34m(arch, block, layers, pretrained, progress, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mResNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m         state_dict = load_state_dict_from_url(model_urls[arch],\n\u001b[0m\u001b[0;32m    223\u001b[0m                                               progress=progress)\n\u001b[0;32m    224\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FYP\\lib\\site-packages\\torch\\hub.py\u001b[0m in \u001b[0;36mload_state_dict_from_url\u001b[1;34m(url, model_dir, map_location, progress, check_hash)\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Downloading: \"{}\" to {}\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcached_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[0mhash_prefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHASH_REGEX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcheck_hash\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m         \u001b[0mdownload_url_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcached_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhash_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;31m# Note: extractall() defaults to overwrite file if exists. No need to clean up beforehand.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FYP\\lib\\site-packages\\torch\\hub.py\u001b[0m in \u001b[0;36mdownload_url_to_file\u001b[1;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[0;32m    408\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhash_prefix\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m             \u001b[0msha256\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msha256\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 410\u001b[1;33m         with tqdm(total=file_size, disable=not progress,\n\u001b[0m\u001b[0;32m    411\u001b[0m                   unit='B', unit_scale=True, unit_divisor=1024) as pbar:\n\u001b[0;32m    412\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FYP\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[0munit_scale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0munit_scale\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_printer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mncols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpbar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdisplay_here\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FYP\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;31m# Prepare IPython progress bar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mIProgress\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# #187 #451 #558 #872\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             raise ImportError(\n\u001b[0m\u001b[0;32m    113\u001b[0m                 \u001b[1;34m\"IProgress not found. Please update jupyter and ipywidgets.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m                 \u001b[1;34m\" See https://ipywidgets.readthedocs.io/en/stable\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "from jonas_net import AlbuNet3D34\n",
    "\n",
    "net = AlbuNet3D34(num_classes=4, pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "overall-master",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_files_2020(base_dir):\n",
    "    \"\"\"\n",
    "    returns a list of lists containing the filenames. The outer list contains all training examples. Each entry in the\n",
    "    outer list is again a list pointing to the files of that training example in the following order:\n",
    "    T1, T1c, T2, FLAIR, segmentation\n",
    "    :param base_dir:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    list_of_lists = []\n",
    "    # current_directory = join(base_dir, glioma_type)\n",
    "    current_directory = base_dir\n",
    "    print(\"Current dir: \",current_directory)\n",
    "    patients = subfolders(current_directory, join=False)\n",
    "    for p in patients:\n",
    "        patient_directory = join(current_directory, p)\n",
    "        t1_file = join(patient_directory, p + \"_t1.nii.gz\")\n",
    "        t1c_file = join(patient_directory, p + \"_t1ce.nii.gz\")\n",
    "        t2_file = join(patient_directory, p + \"_t2.nii.gz\")\n",
    "        flair_file = join(patient_directory, p + \"_flair.nii.gz\")\n",
    "        seg_file = join(patient_directory, p + \"_seg.nii.gz\")\n",
    "        this_case = [t1_file, t1c_file, t2_file, flair_file, seg_file]\n",
    "        # this_case = [t1_file, t1c_file, t2_file, flair_file]\n",
    "        assert all((isfile(i) for i in this_case)), \"some file is missing for patient %s; make sure the following \" \\\n",
    "                                                    \"files are there: %s\" % (p, str(this_case))\n",
    "        list_of_lists.append(this_case)\n",
    "    print(\"Found %d patients\" % len(list_of_lists))\n",
    "    return list_of_lists\n",
    "\n",
    "def get_list_of_files_2020_val(base_dir):\n",
    "    \"\"\"\n",
    "    returns a list of lists containing the filenames. The outer list contains all training examples. Each entry in the\n",
    "    outer list is again a list pointing to the files of that training example in the following order:\n",
    "    T1, T1c, T2, FLAIR, segmentation\n",
    "    :param base_dir:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    list_of_lists = []\n",
    "    # current_directory = join(base_dir, glioma_type)\n",
    "    current_directory = base_dir\n",
    "    print(\"Current dir: \",current_directory)\n",
    "    patients = subfolders(current_directory, join=False)\n",
    "    for p in patients:\n",
    "        patient_directory = join(current_directory, p)\n",
    "        t1_file = join(patient_directory, p + \"_t1.nii.gz\")\n",
    "        t1c_file = join(patient_directory, p + \"_t1ce.nii.gz\")\n",
    "        t2_file = join(patient_directory, p + \"_t2.nii.gz\")\n",
    "        flair_file = join(patient_directory, p + \"_flair.nii.gz\")\n",
    "#         seg_file = join(patient_directory, p + \"_seg.nii.gz\")\n",
    "#         this_case = [t1_file, t1c_file, t2_file, flair_file, seg_file]\n",
    "        this_case = [t1_file, t1c_file, t2_file, flair_file]\n",
    "        assert all((isfile(i) for i in this_case)), \"some file is missing for patient %s; make sure the following \" \\\n",
    "                                                    \"files are there: %s\" % (p, str(this_case))\n",
    "        list_of_lists.append(this_case)\n",
    "    print(\"Found %d patients\" % len(list_of_lists))\n",
    "    return list_of_lists\n",
    "\n",
    "def load_and_preprocess_UPDATED2020(case, patient_name, output_folder):\n",
    "\n",
    "    print(\"Using the new preprocess thingy\")\n",
    "    # print(\"in patient name: \", patient_name)\n",
    "    # print(\"in output folder: \", output_folder)\n",
    "    \"\"\"\n",
    "    loads, preprocesses and saves a case\n",
    "    This is what happens here:\n",
    "    1) load all images and stack them to a 4d array\n",
    "    2) crop to nonzero region, this removes unnecessary zero-valued regions and reduces computation time\n",
    "    3) normalize the nonzero region with its mean and standard deviation\n",
    "    4) save 4d tensor as numpy array. Also save metadata required to create niftis again (required for export\n",
    "    of predictions)\n",
    "\n",
    "    :param case:\n",
    "    :param patient_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # load SimpleITK Images\n",
    "    imgs_sitk = [sitk.ReadImage(i) for i in case]\n",
    "\n",
    "    # get pixel arrays from SimpleITK images\n",
    "    imgs_npy = [sitk.GetArrayFromImage(i) for i in imgs_sitk]\n",
    "\n",
    "    # get some metadata\n",
    "    spacing = imgs_sitk[0].GetSpacing()\n",
    "    # the spacing returned by SimpleITK is in inverse order relative to the numpy array we receive. If we wanted to\n",
    "    # resample the data and if the spacing was not isotropic (in BraTS all cases have already been resampled to 1x1x1mm\n",
    "    # by the organizers) then we need to pay attention here. Therefore we bring the spacing into the correct order so\n",
    "    # that spacing[0] actually corresponds to the spacing of the first axis of the numpy array\n",
    "    spacing = np.array(spacing)[::-1]\n",
    "\n",
    "    direction = imgs_sitk[0].GetDirection()\n",
    "    origin = imgs_sitk[0].GetOrigin()\n",
    "\n",
    "    original_shape = imgs_npy[0].shape\n",
    "\n",
    "    # now stack the images into one 4d array, cast to float because we will get rounding problems if we don't\n",
    "    imgs_npy = np.concatenate([i[None] for i in imgs_npy]).astype(np.float32)\n",
    "\n",
    "    # now find the nonzero region and crop to that\n",
    "    nonzero = [np.array(np.where(i != 0)) for i in imgs_npy]\n",
    "    nonzero = [[np.min(i, 1), np.max(i, 1)] for i in nonzero]\n",
    "    nonzero = np.array([np.min([i[0] for i in nonzero], 0), np.max([i[1] for i in nonzero], 0)]).T\n",
    "    # nonzero now has shape 3, 2. It contains the (min, max) coordinate of nonzero voxels for each axis\n",
    "\n",
    "    # now crop to nonzero\n",
    "    imgs_npy = imgs_npy[:,\n",
    "               nonzero[0, 0] : nonzero[0, 1] + 1,\n",
    "               nonzero[1, 0]: nonzero[1, 1] + 1,\n",
    "               nonzero[2, 0]: nonzero[2, 1] + 1,\n",
    "               ]\n",
    "\n",
    "    # now we create a brain mask that we use for normalization\n",
    "    nonzero_masks = [i != 0 for i in imgs_npy[:-1]]\n",
    "    brain_mask = np.zeros(imgs_npy.shape[1:], dtype=bool)\n",
    "    for i in range(len(nonzero_masks)):\n",
    "        brain_mask = brain_mask | nonzero_masks[i]\n",
    "\n",
    "    # now normalize each modality with its mean and standard deviation (computed within the brain mask)\n",
    "    for i in range(len(imgs_npy) - 1):\n",
    "        mean = imgs_npy[i][brain_mask].mean()\n",
    "        std = imgs_npy[i][brain_mask].std()\n",
    "        imgs_npy[i] = (imgs_npy[i] - mean) / (std + 1e-8)\n",
    "        imgs_npy[i][brain_mask == 0] = 0\n",
    "\n",
    "    # the segmentation of brats has the values 0, 1, 2 and 4. This is pretty inconvenient to say the least.\n",
    "    # We move everything that is 4 to 3\n",
    "    imgs_npy[-1][imgs_npy[-1] == 4] = 3\n",
    "\n",
    "    # now save as npz\n",
    "    np.save(join(output_folder, patient_name + \".npy\"), imgs_npy)\n",
    "    # print(\"Output folder end: \", join(output_folder, patient_name + \".npy\"))\n",
    "    # print(\"\\n\")\n",
    "\n",
    "\n",
    "    metadata = {\n",
    "        'spacing': spacing,\n",
    "        'direction': direction,\n",
    "        'origin': origin,\n",
    "        'original_shape': original_shape,\n",
    "        'nonzero_region': nonzero\n",
    "    }\n",
    "\n",
    "    save_pickle(metadata, join(output_folder, patient_name + \".pkl\"))\n",
    "\n",
    "    \n",
    "def predict_patient_in_patches(patient_data, model):\n",
    "    # we pad the patient data in order to fit the patches in it\n",
    "    patient_data_pd = pad_nd_image(patient_data, [144, 192, 192]) # 24*6, 128+2*32, 128+2*32\n",
    "    # patches.shape = (1, 1, 6, 3, 3, 1, 3, 24, 128, 128)\n",
    "    steps = (1,1,args.patch_depth,int(args.patch_width/4),int(args.patch_height/4))\n",
    "    window_shape = (1, 3, args.patch_depth, args.patch_width, args.patch_height)\n",
    "    patches = skimage.util.view_as_windows(patient_data_pd[:, :3, :, :, :], window_shape=window_shape, step=steps)\n",
    "    \n",
    "    # (1, 4, 138, 169, 141)\n",
    "    target_shape = list(patient_data_pd.shape)\n",
    "    print(f\"Target shape in predict patient in patches is: {target_shape}\")\n",
    "\n",
    "    if args.multi_class:\n",
    "        target_shape[1] = 4\n",
    "    else:\n",
    "        target_shape[1] = 1 # only one output channel\n",
    "    prediction = torch.zeros(*target_shape)\n",
    "    if args.use_gpu:\n",
    "        prediction = prediction.cuda()\n",
    "    \n",
    "    for i in range(patches.shape[2]):\n",
    "        for j in range(patches.shape[3]):\n",
    "            for k in range(patches.shape[4]):\n",
    "                data = torch.from_numpy(patches[0, 0, i, j, k])\n",
    "                if args.use_gpu:\n",
    "                    data = data.cuda()\n",
    "                output = model.forward(data)\n",
    "\n",
    "                prediction[:, :,\n",
    "                           i*steps[2]:i*steps[2]+window_shape[2],\n",
    "                           j*steps[3]:j*steps[3]+window_shape[3],\n",
    "                           k*steps[4]:k*steps[4]+window_shape[4]] += output\n",
    "                    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "sonic-massachusetts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dir:  C:/Users/JiachennCJC/Documents/GitHub/brats-pretraining_jiachenn/MICCAI_BraTS2020_TrainingData/\n",
      "Found 369 patients\n",
      "Current dir:  C:/Users/JiachennCJC/Documents/GitHub/brats-pretraining_jiachenn/MICCAI_BraTS2020_ValidationData/\n",
      "Found 125 patients\n"
     ]
    }
   ],
   "source": [
    "patient_folders_train = get_list_of_files_2020(brats_folder_with_training_data_2020)\n",
    "patient_folders_validation = get_list_of_files_2020_val(brats_folder_with_validation_data_2020)\n",
    "patient_names_train = [i[0].split('\\\\')[-2] for i in patient_folders_train] \n",
    "patient_names_valid = [i[0].split('\\\\')[-2] for i in patient_folders_validation] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "american-trinidad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/JiachennCJC/Documents/GitHub/brats-pretraining_jiachenn/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001\\\\BraTS20_Training_001_t1.nii.gz',\n",
       " 'C:/Users/JiachennCJC/Documents/GitHub/brats-pretraining_jiachenn/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001\\\\BraTS20_Training_001_t1ce.nii.gz',\n",
       " 'C:/Users/JiachennCJC/Documents/GitHub/brats-pretraining_jiachenn/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001\\\\BraTS20_Training_001_t2.nii.gz',\n",
       " 'C:/Users/JiachennCJC/Documents/GitHub/brats-pretraining_jiachenn/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001\\\\BraTS20_Training_001_flair.nii.gz',\n",
       " 'C:/Users/JiachennCJC/Documents/GitHub/brats-pretraining_jiachenn/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001\\\\BraTS20_Training_001_seg.nii.gz']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case = patient_folders_train[0:2]\n",
    "case[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "seventh-davis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/JiachennCJC/Documents/GitHub/brats-pretraining_jiachenn/MICCAI_BraTS2020_ValidationData/BraTS20_Validation_001\\\\BraTS20_Validation_001_t1.nii.gz',\n",
       " 'C:/Users/JiachennCJC/Documents/GitHub/brats-pretraining_jiachenn/MICCAI_BraTS2020_ValidationData/BraTS20_Validation_001\\\\BraTS20_Validation_001_t1ce.nii.gz',\n",
       " 'C:/Users/JiachennCJC/Documents/GitHub/brats-pretraining_jiachenn/MICCAI_BraTS2020_ValidationData/BraTS20_Validation_001\\\\BraTS20_Validation_001_t2.nii.gz',\n",
       " 'C:/Users/JiachennCJC/Documents/GitHub/brats-pretraining_jiachenn/MICCAI_BraTS2020_ValidationData/BraTS20_Validation_001\\\\BraTS20_Validation_001_flair.nii.gz']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case = patient_folders_validation[0:2]\n",
    "case[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "expressed-substance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 155, 240, 240)\n",
      "(155, 240, 240)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "# Training data read\n",
    "test = sitk.ReadImage(case[0])\n",
    "test_np_array = sitk.GetArrayFromImage(test)\n",
    "\n",
    "test_np_array = np.concatenate([i[None] for i in test_np_array]).astype(np.float32)\n",
    "print(test_np_array.shape)\n",
    "print(test_np_array[-1].shape)\n",
    "print(test_np_array[-1].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "stopped-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.zeros([240,240,155],dtype=\"float32\")\n",
    "test.shape\n",
    "img = nib.Nifti1Image(test, np.eye(4))\n",
    "img.get_fdata().shape\n",
    "nib.save(img, \"TESTINGDUMMYSEG.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "advisory-minute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155, 240, 240)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nib.load(\"TESTINGDUMMYSEG.nii.gz\").get_fdata().shape\n",
    "dummy_img = sitk.GetArrayFromImage(sitk.ReadImage(\"TESTINGDUMMYSEG.nii.gz\"))\n",
    "dummy_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "alone-miami",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[  0,   0,   0, ..., 154, 154, 154],\n",
      "       [  0,   0,   0, ..., 239, 239, 239],\n",
      "       [  0,   1,   2, ..., 237, 238, 239]], dtype=int64), array([[  0,   0,   0, ..., 154, 154, 154],\n",
      "       [  0,   0,   0, ..., 239, 239, 239],\n",
      "       [  0,   1,   2, ..., 237, 238, 239]], dtype=int64), array([[  0,   0,   0, ..., 154, 154, 154],\n",
      "       [  0,   0,   0, ..., 239, 239, 239],\n",
      "       [  0,   1,   2, ..., 237, 238, 239]], dtype=int64), array([[  0,   0,   0, ..., 154, 154, 154],\n",
      "       [  0,   0,   0, ..., 239, 239, 239],\n",
      "       [  0,   1,   2, ..., 237, 238, 239]], dtype=int64), array([[  0,   0,   0, ..., 154, 154, 154],\n",
      "       [  0,   0,   0, ..., 239, 239, 239],\n",
      "       [  0,   1,   2, ..., 237, 238, 239]], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib \n",
    "pat305 = nib.load(\"MICCAI_BraTS2020_TrainingData/BraTS20_Training_305/BraTS20_Training_305_seg.nii.gz\").get_fdata()\n",
    "print([np.array(np.where(i == 0)) for i in test_np_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "intelligent-hearing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([91, 91, 91, ..., 99, 91, 92], dtype=int64)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nonzero(pat305)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "offshore-associate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23ae56bca00>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(pat305[:,:,97])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "roman-cartoon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 4]\n",
      "(155, 240, 240)\n"
     ]
    }
   ],
   "source": [
    "model_folder = \"20210311-070028_brats20_3d_pretrained_trainbatchsize_200_lr_0.001_epochs_50_epochbatch_200_epoch_30\"\n",
    "test = sitk.ReadImage(f\"segmentation_output/Wrong Preprocessing Seg Output/{model_folder}/Brats20ValidationData/BraTS20_Validation_061.nii.gz\")\n",
    "new = sitk.GetArrayFromImage(test)\n",
    "print(np.unique(new))\n",
    "print(new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "annoying-delight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 4]\n",
      "(155, 240, 240)\n"
     ]
    }
   ],
   "source": [
    "model_folder = \"20210309-133958_brats20_3d_pretrained_new_lr_1_lr_0.0001_epochs_60_TESTINGONTRAINING_40\"\n",
    "test = sitk.ReadImage(f\"segmentation_output/Wrong Preprocessing Seg Output/{model_folder}/Brats20TrainingData/BraTS20_Training_061.nii.gz\")\n",
    "new = sitk.GetArrayFromImage(test)\n",
    "print(np.unique(new))\n",
    "print(new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "civilian-speaking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 4]\n",
      "(155, 240, 240)\n"
     ]
    }
   ],
   "source": [
    "model_folder=\"20210321-070858_brats20_3d_small_training_2_lr_0.0001_epochs_70_epochbatch_75_epoch_UPGRADEDTRAININGDUMMY_10\"\n",
    "test = sitk.ReadImage(f\"segmentation_output/{model_folder}/Brats20TrainingData/BraTS20_Training_301.nii.gz\")\n",
    "new = sitk.GetArrayFromImage(test)\n",
    "print(np.unique(new))\n",
    "print(new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "limited-tomato",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 45,  45,  45, ..., 133, 133, 133], dtype=int64),\n",
       " array([109, 110, 110, ..., 131, 131, 135], dtype=int64),\n",
       " array([139, 138, 142, ..., 137, 139, 143], dtype=int64))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "stuffed-eating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 4]\n",
      "(155, 240, 240)\n"
     ]
    }
   ],
   "source": [
    "model_folder = \"20210321-095418_brats20_3d_pretrained_trainbatchsize_100_2_lr_0.0001_epochs_50_epochbatch_100_epoch_UPDATEDVALIDATION_30\"\n",
    "test = sitk.ReadImage(f\"segmentation_output/{model_folder}/Brats20ValidationData/BraTS20_Validation_076.nii.gz\")\n",
    "new = sitk.GetArrayFromImage(test)\n",
    "print(np.unique(new))\n",
    "print(new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "devoted-frequency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 4]\n",
      "(155, 240, 240)\n"
     ]
    }
   ],
   "source": [
    "test = sitk.ReadImage(\"segmentation_output/20210321-095418_brats20_3d_pretrained_trainbatchsize_100_2_lr_0.0001_epochs_50_epochbatch_100_epoch_UPDATEDVALIDATION_30/Brats20ValidationData/BraTS20_Validation_001.nii.gz\")\n",
    "new = sitk.GetArrayFromImage(test)\n",
    "print(np.unique(new))\n",
    "print(new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "quiet-precipitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 4]\n",
      "(155, 240, 240)\n"
     ]
    }
   ],
   "source": [
    "model_folder = \"20210309-021115_brats20_3d_pretrained_new_1_lr_0.01_epochs_60_40\"\n",
    "test = sitk.ReadImage(f\"segmentation_output/Wrong Preprocessing Seg Output/{model_folder}/Brats20ValidationData/BraTS20_Validation_061.nii.gz\")\n",
    "new = sitk.GetArrayFromImage(test)\n",
    "print(np.unique(new))\n",
    "print(new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "square-assault",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155, 240, 240)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "assigned-savannah",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 4], dtype=uint8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "monetary-german",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 4], dtype=uint8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "spectacular-instruction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f95ac10160>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAMY0lEQVR4nO3dQYyc9X2H8edrYxYFqIJLoK7tBhM5bc2hDlqZqFQRFSo49GByIDWH1AdU9wBSIrUHk1QKF9S0UugNIkdBcasU14IgfEAtxImEqlQBExmwcQwLOHixa4cSNShIBptfD/tamZpdr72z45n1//lIo3nnP+/M/HhtP7wzO4ZUFZLatWjYA0gaLiMgNc4ISI0zAlLjjIDUOCMgNW5gEUiyPsmBJBNJtgzqdST1J4P4nkCSxcArwJ8Bk8BzwJ1V9fK8v5ikvgzqTGAdMFFVr1fV+8B2YMOAXktSHy4a0PMuBw713J4Ebphp54szVpdw6YBGkQTwLr98u6o+cfr6oCKQadb+3/uOJJuBzQCX8DFuyM0DGkUSwA/q0Z9Ptz6otwOTwMqe2yuAw707VNXWqhqvqvEljA1oDEmzGVQEngNWJ1mV5GJgI7BzQK8lqQ8DeTtQVSeS3AP8B7AYeLiq9g3itST1Z1CfCVBVTwJPDur5Jc0PvzEoNc4ISI0zAlLjjIDUOCMgNc4ISI0zAlLjjIDUOCMgNc4ISI0zAlLjjIDUOCMgNc4ISI0zAlLjjIDUOCMgNc4ISI0zAlLjjIDUOCMgNc4ISI0zAlLjjIDUOCMgNc4ISI0zAlLjjIDUOCMgNc4ISI0zAlLjjIDUOCMgNc4ISI0zAlLjjIDUuIv6eXCSg8C7wEngRFWNJ1kK/BtwDXAQ+GJV/bK/MSUNynycCfxpVa2tqvHu9hZgV1WtBnZ1tyWNqEG8HdgAbOu2twG3D+A1JM2TfiNQwFNJnk+yuVu7uqqOAHTXV/X5GpIGqK/PBIAbq+pwkquAp5P87Gwf2EVjM8AlfKzPMSTNVV9nAlV1uLs+BjwOrAOOJlkG0F0fm+GxW6tqvKrGlzDWzxiS+jDnCCS5NMnlp7aBW4C9wE5gU7fbJuCJfoeUNDj9vB24Gng8yann+deq+vckzwE7ktwFvAnc0f+YkgZlzhGoqteBP5pm/X+Am/sZStL54zcGpcYZAalxRkBqnBGQGmcEpMYZAalxRkBqnBGQGmcEpMYZAalxRkBqnBGQGmcEpMYZAalxRkBqnBGQGmcEpMYZAalxRkBqnBGQGmcEpMYZAalxRkBqnBGQGmcEpMYZAalxRkBqnBGQGmcEpMYZAalxRkBqnBGQGmcEpMYZAalxRkBqnBGQGmcEpMbNGoEkDyc5lmRvz9rSJE8nebW7vqLnvnuTTCQ5kOTWQQ0uaX6czZnAd4H1p61tAXZV1WpgV3ebJGuAjcB13WMeTLJ43qaVNO9mjUBVPQO8c9ryBmBbt70NuL1nfXtVHa+qN4AJYN38jCppEOb6mcDVVXUEoLu+qltfDhzq2W+yW/uIJJuT7E6y+wOOz3EMSf2a7w8GM81aTbdjVW2tqvGqGl/C2DyPIelszTUCR5MsA+iuj3Xrk8DKnv1WAIfnPp6kQZtrBHYCm7rtTcATPesbk4wlWQWsBp7tb0RJg3TRbDskeQS4CbgyySTwdeAbwI4kdwFvAncAVNW+JDuAl4ETwN1VdXJAs0uaB7NGoKrunOGum2fY/37g/n6GknT++I1BqXFGQGqcEZAaZwSkxhkBqXFGQGqcEZAaZwSkxhkBqXFGQGqcEZAaZwSkxhkBqXFGQGqcEZAaZwSkxhkBqXFGQGqcEZAaZwSkxhkBqXFGQGqcEZAaZwSkxhkBqXFGQGqcEZAaZwSkxhkBqXFGQGqcEZAaZwSkxhkBqXFGQGqcEZAaZwSkxs0agSQPJzmWZG/P2n1J3kqyp7vc1nPfvUkmkhxIcuugBpc0P87mTOC7wPpp1v+pqtZ2lycBkqwBNgLXdY95MMni+RpW0vybNQJV9Qzwzlk+3wZge1Udr6o3gAlgXR/zSRqwfj4TuCfJi93bhSu6teXAoZ59Jrs1SSNqrhF4CPgUsBY4AnyzW880+9Z0T5Bkc5LdSXZ/wPE5jiGpX3OKQFUdraqTVfUh8G1+c8o/Cazs2XUFcHiG59haVeNVNb6EsbmMIWkezCkCSZb13PwCcOonBzuBjUnGkqwCVgPP9jeipEG6aLYdkjwC3ARcmWQS+DpwU5K1TJ3qHwT+GqCq9iXZAbwMnADurqqTA5lc0rxI1bRv2c+r38rSuiE3D3sM6YL2g3r0+aoaP33dbwxKjTMCUuOMgNQ4IyA1zghIjTMCUuOMgNQ4IyA1zghIjTMCUuOMgNQ4IyA1zghIjTMCUuOMgNQ4IyA1zghIjTMCUuOMgNQ4IyA1zghIjZv1Pzmu0TbxwGchcMmxRaz4+x8PexwtQJ4JLHCXvrWI1/7iW9x55w+HPYoWKCOwwC1/aA8Af/nx3bzyoP8DaJ07I3CB+L2LLuP3/+CtYY+hBcgILHAfvvce6zd8CYBHP/0Yr3zLswGdGyNwAVj8i//lvQ/f57JFl7Do0g+GPY4WGCNwAThx8E1u/+Jf8fzx9/nw10uGPY4WGH9EeIHIj1/gq6vWseZ3fg7Lf5cTbx0e9khaIDwTuMC8fcu1/Peff3LYY2gB8UzgAvPxf/6vYY+gBcYzAalxRuACdejv/phFa9cMewwtAEbgArXyqXfJ5NFhj6EFwM8ELlTPvsTJYc+gBcEzAalxs0YgycokP0qyP8m+JF/u1pcmeTrJq931FT2PuTfJRJIDSW4d5D+ApP6czZnACeBvquoPgc8CdydZA2wBdlXVamBXd5vuvo3AdcB64MEkiwcxvKT+zRqBqjpSVT/ttt8F9gPLgQ3Atm63bcDt3fYGYHtVHa+qN4AJwL/VIo2oc/pMIMk1wGeAnwBXV9URmAoFcFW323LgUM/DJrs1SSPorCOQ5DLgMeArVfWrM+06zVpN83ybk+xOsvsDjp/tGJLm2VlFIMkSpgLwvar6frd8NMmy7v5lwLFufRJY2fPwFcBH/jZLVW2tqvGqGl/C2Fznl9Sns/npQIDvAPur6oGeu3YCm7rtTcATPesbk4wlWQWsBp6dv5Elzaez+bLQjcCXgJeS7OnWvgp8A9iR5C7gTeAOgKral2QH8DJTP1m4u6r83oo0omaNQFX9J9O/zwe4eYbH3A/c38dcks4TvzEoNc4ISI0zAlLjjIDUOCMgNc4ISI0zAlLjjIDUOCMgNc4ISI0zAlLjjIDUOCMgNc4ISI0zAlLjjIDUOCMgNc4ISI0zAlLjjIDUOCMgNc4ISI0zAlLjjIDUOCMgNc4ISI0zAlLjjIDUOCMgNc4ISI0zAlLjUlXDnoEkvwB+Dbw97FnO0ZU48/myEOcetZk/WVWfOH1xJCIAkGR3VY0Pe45z4cznz0Kce6HM7NsBqXFGQGrcKEVg67AHmANnPn8W4twLYuaR+UxA0nCM0pmApCEYegSSrE9yIMlEki3DnudMkhxM8lKSPUl2d2tLkzyd5NXu+oohz/hwkmNJ9vaszThjknu7Y38gya0jNPN9Sd7qjvWeJLeN2Mwrk/woyf4k+5J8uVsf6WM9raoa2gVYDLwGXAtcDLwArBnmTLPMexC48rS1fwS2dNtbgH8Y8oyfA64H9s42I7CmO+ZjwKru12LxiMx8H/C30+w7KjMvA67vti8HXulmG+ljPd1l2GcC64CJqnq9qt4HtgMbhjzTudoAbOu2twG3D28UqKpngHdOW55pxg3A9qo6XlVvABNM/ZqcVzPMPJNRmflIVf20234X2A8sZ8SP9XSGHYHlwKGe25Pd2qgq4KkkzyfZ3K1dXVVHYOo3BnDV0Kab2UwzjvrxvyfJi93bhVOn1SM3c5JrgM8AP2EBHuthRyDTrI3yjyturKrrgc8Ddyf53LAH6tMoH/+HgE8Ba4EjwDe79ZGaOcllwGPAV6rqV2fadZq1kTjWw47AJLCy5/YK4PCQZplVVR3uro8BjzN1Onc0yTKA7vrY8Cac0Uwzjuzxr6qjVXWyqj4Evs1vTp1HZuYkS5gKwPeq6vvd8oI71sOOwHPA6iSrklwMbAR2DnmmaSW5NMnlp7aBW4C9TM27qdttE/DEcCY8o5lm3AlsTDKWZBWwGnh2CPN9xKk/SJ0vMHWsYURmThLgO8D+qnqg564Fd6yH/skkcBtTn6y+Bnxt2POcYc5rmfp09wVg36lZgd8GdgGvdtdLhzznI0ydPn/A1L997jrTjMDXumN/APj8CM38L8BLwItM/QFaNmIz/wlTp/MvAnu6y22jfqynu/iNQalxw347IGnIjIDUOCMgNc4ISI0zAlLjjIDUOCMgNc4ISI37P7B1AKgSmT/yAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(new[65,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "quantitative-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.ones([240,240,155],dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "representative-receptor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dir:  C:/Users/JiachennCJC/Documents/GitHub/brats-pretraining_jiachenn/MICCAI_BraTS2020_TrainingData_DUMMY/\n",
      "Found 69 patients\n"
     ]
    }
   ],
   "source": [
    "# Try to create dummy files lmao\n",
    "brats_folder_with_dummy_data_2020 = f\"{base_path}MICCAI_BraTS2020_TrainingData_DUMMY/\"\n",
    " \n",
    "list_of_patients = get_list_of_files_2020(brats_folder_with_dummy_data_2020)\n",
    "patient_names_training = [i[0].split('/')[-1].split('\\\\')[0] for i in list_of_patients] \n",
    "patient_names_validation = [i[0].split('/')[-1].split('\\\\')[0] for i in patient_folders_validation] \n",
    "\n",
    "# patient_names_training = patient_names_training[0:5]\n",
    "\n",
    "for i in patient_names_training:\n",
    "    test = np.ones([240,240,155],dtype=\"float32\")\n",
    "    img = nib.Nifti1Image(test, np.eye(4))\n",
    "#     nib.save(img, f\"{brats_folder_with_dummy_data_2020}\"+f\"{i}/\"+ f\"{i}_segUPDATE.nii.gz\")\n",
    "\n",
    "sitk.GetArrayFromImage(sitk.ReadImage(brats_folder_with_dummy_data_2020 + \"BraTS20_Training_301/BraTS20_Training_301_segUPDATE.nii.gz\")).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "serial-creation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 155, 240, 240)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation data read\n",
    "test = sitk.ReadImage(case[0])\n",
    "sitk.GetArrayFromImage(test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "disabled-friday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/JiachennCJC/Documents/GitHub/brats-pretraining_jiachenn/MICCAI_BraTS2020_TrainingData/BraTS20_Training_002\\\\BraTS20_Training_002_t1.nii.gz',\n",
       " 'C:/Users/JiachennCJC/Documents/GitHub/brats-pretraining_jiachenn/MICCAI_BraTS2020_TrainingData/BraTS20_Training_002\\\\BraTS20_Training_002_t1ce.nii.gz',\n",
       " 'C:/Users/JiachennCJC/Documents/GitHub/brats-pretraining_jiachenn/MICCAI_BraTS2020_TrainingData/BraTS20_Training_002\\\\BraTS20_Training_002_t2.nii.gz',\n",
       " 'C:/Users/JiachennCJC/Documents/GitHub/brats-pretraining_jiachenn/MICCAI_BraTS2020_TrainingData/BraTS20_Training_002\\\\BraTS20_Training_002_flair.nii.gz',\n",
       " 'C:/Users/JiachennCJC/Documents/GitHub/brats-pretraining_jiachenn/MICCAI_BraTS2020_TrainingData/BraTS20_Training_002\\\\BraTS20_Training_002_seg.nii.gz']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_list = patient_folders_train[0:2]\n",
    "case = short_list[1]\n",
    "case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "moral-farming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape is: (155, 240, 240)\n",
      "After concat shape is: (5, 155, 240, 240)\n",
      "Nonzero shape is (3, 2)\n",
      "Nonzero value is [[  0 139]\n",
      " [ 31 217]\n",
      " [ 53 186]]\n",
      "After cropping nonzero shape is : (5, 140, 187, 134)\n",
      "Brain mask shape: (140, 187, 134)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Imgs npy after normalizing: (5, 140, 187, 134)\n"
     ]
    }
   ],
   "source": [
    "imgs_sitk = [sitk.ReadImage(i) for i in case]\n",
    "\n",
    "imgs_npy = [sitk.GetArrayFromImage(i) for i in imgs_sitk]\n",
    "\n",
    "# get some metadata\n",
    "spacing = imgs_sitk[0].GetSpacing()\n",
    "\n",
    "# the spacing returned by SimpleITK is in inverse order relative to the numpy array we receive. If we wanted to\n",
    "# resample the data and if the spacing was not isotropic (in BraTS all cases have already been resampled to 1x1x1mm\n",
    "# by the organizers) then we need to pay attention here. Therefore we bring the spacing into the correct order so\n",
    "# that spacing[0] actually corresponds to the spacing of the first axis of the numpy array\n",
    "spacing = np.array(spacing)[::-1]\n",
    "\n",
    "direction = imgs_sitk[0].GetDirection()\n",
    "origin = imgs_sitk[0].GetOrigin()\n",
    "\n",
    "original_shape = imgs_npy[0].shape\n",
    "print(f\"Original shape is: {original_shape}\")\n",
    "\n",
    "imgs_npy = np.concatenate([i[None] for i in imgs_npy]).astype(np.float32)\n",
    "print(f\"After concat shape is: {imgs_npy.shape}\")\n",
    "\n",
    "# now find the nonzero region and crop to that\n",
    "nonzero = [np.array(np.where(i != 0)) for i in imgs_npy]\n",
    "nonzero = [[np.min(i, 1), np.max(i, 1)] for i in nonzero]\n",
    "nonzero = np.array([np.min([i[0] for i in nonzero], 0), np.max([i[1] for i in nonzero], 0)]).T\n",
    "print(f\"Nonzero shape is {nonzero.shape}\")\n",
    "print(f\"Nonzero value is {nonzero}\")\n",
    "# nonzero now has shape 3, 2. It contains the (min, max) coordinate of nonzero voxels for each axis\n",
    "\n",
    "# now crop to nonzero, note that this applies the crop to all channels (t1,t1ce,flair,etc)\n",
    "imgs_npy = imgs_npy[:,\n",
    "           nonzero[0, 0] : nonzero[0, 1] + 1,\n",
    "           nonzero[1, 0]: nonzero[1, 1] + 1,\n",
    "           nonzero[2, 0]: nonzero[2, 1] + 1,\n",
    "           ]\n",
    "print(f\"After cropping nonzero shape is : {imgs_npy.shape}\")\n",
    "\n",
    "# now we create a brain mask that we use for normalization \n",
    "# this excludes the seg mask since imgs_npy[:-1] means its not covered\n",
    "nonzero_masks = [i != 0 for i in imgs_npy[:-1]]\n",
    "brain_mask = np.zeros(imgs_npy.shape[1:], dtype=bool)\n",
    "\n",
    "print(f\"Brain mask shape: {brain_mask.shape}\")\n",
    "\n",
    "for i in range(len(nonzero_masks)):\n",
    "    brain_mask = brain_mask | nonzero_masks[i]\n",
    "    \n",
    "# now normalize each modality with its mean and standard deviation (computed within the brain mask)\n",
    "# This also excludes normalizing the last element of the list (seg if 5 files)\n",
    "for i in range(len(imgs_npy) - 1):\n",
    "    print(i)\n",
    "    mean = imgs_npy[i][brain_mask].mean()\n",
    "    std = imgs_npy[i][brain_mask].std()\n",
    "    imgs_npy[i] = (imgs_npy[i] - mean) / (std + 1e-8)\n",
    "    imgs_npy[i][brain_mask == 0] = 0\n",
    "\n",
    "print(f\"Imgs npy after normalizing: {imgs_npy.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-preparation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "military-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_npy[-1][imgs_npy[-1]==3]=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "answering-international",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 4., 4., ..., 4., 4., 4.], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_npy[-1][imgs_npy[-1]==4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "executive-emphasis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3.], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(imgs_npy[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "literary-luxembourg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(imgs_npy)):\n",
    "    print(i)\n",
    "    mean = imgs_npy[i][brain_mask].mean()\n",
    "    std = imgs_npy[i][brain_mask].std()\n",
    "    imgs_npy[i] = (imgs_npy[i] - mean) / (std + 1e-8)\n",
    "    imgs_npy[i][brain_mask == 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "presidential-roommate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imgs_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "sought-adolescent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, ..., 4, 4, 4], dtype=int64),\n",
       " array([  0,   0,   0, ..., 108, 108, 108], dtype=int64),\n",
       " array([105, 106, 106, ...,  87,  88,  88], dtype=int64),\n",
       " array([72, 69, 70, ..., 53, 52, 53], dtype=int64))"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_npy.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "skilled-exposure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(10)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "obvious-perfume",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 135, 173, 137)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_npy[:-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "twenty-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brats_data_loader import *\n",
    "\n",
    "patdata, metadata = BRATSDataLoader.load_patient(\"brats_data_preprocessed/Brats20ValidationData/BraTS20_Validation_001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ordinary-accident",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 135, 173, 137) {'spacing': array([1., 1., 1.]), 'direction': (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0), 'origin': (-0.0, -239.0, 0.0), 'original_shape': (155, 240, 240), 'nonzero_region': array([[  4, 138],\n",
      "       [ 42, 214],\n",
      "       [ 49, 185]], dtype=int64)}\n",
      "[array([[  4,   4,   4, ..., 138, 138, 138],\n",
      "       [147, 148, 148, ..., 124, 124, 125],\n",
      "       [121, 118, 119, ..., 131, 132, 131]], dtype=int64), array([[  4,   4,   4, ..., 138, 138, 138],\n",
      "       [147, 148, 148, ..., 124, 124, 125],\n",
      "       [121, 118, 119, ..., 131, 132, 131]], dtype=int64), array([[  4,   4,   4, ..., 138, 138, 138],\n",
      "       [147, 148, 148, ..., 124, 124, 125],\n",
      "       [121, 118, 119, ..., 131, 132, 131]], dtype=int64), array([[  4,   4,   4, ..., 138, 138, 138],\n",
      "       [147, 148, 148, ..., 124, 124, 125],\n",
      "       [121, 118, 119, ..., 131, 132, 131]], dtype=int64), array([[ 30,  30,  30, ..., 112, 112, 112],\n",
      "       [107, 107, 108, ..., 129, 130, 130],\n",
      "       [ 93,  94,  93, ..., 102, 101, 102]], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "patdata, metadata = BRATSDataLoader.load_patient(\"brats_data_preprocessed/Brats20TrainingData/BraTS20_Training_001\")\n",
    "print(patdata.shape, metadata)\n",
    "print([np.array(np.where(i != 0)) for i in test_np_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_and_preprocess_UPDATED2020(case, patient_name, output_folder):\n",
    "\"\"\"\n",
    "loads, preprocesses and saves a case\n",
    "This is what happens here:\n",
    "1) load all images and stack them to a 4d array\n",
    "2) crop to nonzero region, this removes unnecessary zero-valued regions and reduces computation time\n",
    "3) normalize the nonzero region with its mean and standard deviation\n",
    "4) save 4d tensor as numpy array. Also save metadata required to create niftis again (required for export\n",
    "of predictions)\n",
    "\n",
    ":param case: list of lists of directory\n",
    ":param patient_name: list of patient name (strings)\n",
    ":return:\n",
    "\"\"\"\n",
    "# load SimpleITK Images\n",
    "imgs_sitk = [sitk.ReadImage(i) for i in case]\n",
    "\n",
    "# get pixel arrays from SimpleITK images\n",
    "imgs_npy = [sitk.GetArrayFromImage(i) for i in imgs_sitk]\n",
    "\n",
    "# get some metadata\n",
    "spacing = imgs_sitk[0].GetSpacing()\n",
    "# the spacing returned by SimpleITK is in inverse order relative to the numpy array we receive. If we wanted to\n",
    "# resample the data and if the spacing was not isotropic (in BraTS all cases have already been resampled to 1x1x1mm\n",
    "# by the organizers) then we need to pay attention here. Therefore we bring the spacing into the correct order so\n",
    "# that spacing[0] actually corresponds to the spacing of the first axis of the numpy array\n",
    "spacing = np.array(spacing)[::-1]\n",
    "\n",
    "direction = imgs_sitk[0].GetDirection()\n",
    "origin = imgs_sitk[0].GetOrigin()\n",
    "\n",
    "original_shape = imgs_npy[0].shape\n",
    "\n",
    "# now stack the images into one 4d array, cast to float because we will get rounding problems if we don't\n",
    "imgs_npy = np.concatenate([i[None] for i in imgs_npy]).astype(np.float32)\n",
    "\n",
    "# now find the nonzero region and crop to that\n",
    "nonzero = [np.array(np.where(i != 0)) for i in imgs_npy]\n",
    "nonzero = [[np.min(i, 1), np.max(i, 1)] for i in nonzero]\n",
    "nonzero = np.array([np.min([i[0] for i in nonzero], 0), np.max([i[1] for i in nonzero], 0)]).T\n",
    "# nonzero now has shape 3, 2. It contains the (min, max) coordinate of nonzero voxels for each axis\n",
    "\n",
    "# now crop to nonzero\n",
    "imgs_npy = imgs_npy[:,\n",
    "           nonzero[0, 0] : nonzero[0, 1] + 1,\n",
    "           nonzero[1, 0]: nonzero[1, 1] + 1,\n",
    "           nonzero[2, 0]: nonzero[2, 1] + 1,\n",
    "           ]\n",
    "\n",
    "# now we create a brain mask that we use for normalization\n",
    "nonzero_masks = [i != 0 for i in imgs_npy[:-1]]\n",
    "brain_mask = np.zeros(imgs_npy.shape[1:], dtype=bool)\n",
    "for i in range(len(nonzero_masks)):\n",
    "    brain_mask = brain_mask | nonzero_masks[i]\n",
    "\n",
    "# now normalize each modality with its mean and standard deviation (computed within the brain mask)\n",
    "for i in range(len(imgs_npy) - 1):\n",
    "    mean = imgs_npy[i][brain_mask].mean()\n",
    "    std = imgs_npy[i][brain_mask].std()\n",
    "    imgs_npy[i] = (imgs_npy[i] - mean) / (std + 1e-8)\n",
    "    imgs_npy[i][brain_mask == 0] = 0\n",
    "\n",
    "# the segmentation of brats has the values 0, 1, 2 and 4. This is pretty inconvenient to say the least.\n",
    "# We move everything that is 4 to 3\n",
    "imgs_npy[-1][imgs_npy[-1] == 4] = 3\n",
    "\n",
    "# now save as npz\n",
    "# np.save(join(output_folder, patient_name + \".npy\"), imgs_npy)\n",
    "# print(\"Output folder end: \", join(output_folder, patient_name + \".npy\"))\n",
    "# print(\"\\n\")\n",
    "\n",
    "\n",
    "metadata = {\n",
    "    'spacing': spacing,\n",
    "    'direction': direction,\n",
    "    'origin': origin,\n",
    "    'original_shape': original_shape,\n",
    "    'nonzero_region': nonzero\n",
    "}\n",
    "\n",
    "# save_pickle(metadata, join(output_folder, patient_name + \".pkl\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
